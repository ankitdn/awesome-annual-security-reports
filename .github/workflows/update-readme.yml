name: Update README with New Reports

on:
  workflow_run:
    workflows: ["PDF to Markdown Conversion"]
    types:
      - completed
    branches: [main, development]
  workflow_dispatch:

jobs:
  update-readme:
    if: ${{ github.event_name != 'workflow_run' || github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install google-generativeai pyyaml mistune gitpython requests
      
      - name: Determine current branch
        id: branch
        run: |
          CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD || echo ${GITHUB_REF#refs/heads/})
          echo "Current branch: $CURRENT_BRANCH"
          echo "branch=$CURRENT_BRANCH" >> $GITHUB_OUTPUT
      
      - name: Download workflow artifact
        if: github.event_name == 'workflow_run'
        uses: actions/github-script@v7
        id: download
        with:
          script: |
            const fs = require('fs');
            
            // Get the workflow run that triggered this workflow
            const runId = context.payload.workflow_run.id;
            console.log(`Processing workflow run: ${runId}`);
            
            try {
              // Get artifacts from the workflow run
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: runId
              });
              
              // Find the artifact containing the converted file info
              const artifact = artifacts.data.artifacts.find(artifact => artifact.name === 'converted_file_info');
              
              if (!artifact) {
                console.log('No converted_file_info artifact found. This may be normal if no files were processed.');
                return 'no_artifact';
              }
              
              // Download the artifact
              const download = await github.rest.actions.downloadArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id,
                archive_format: 'zip'
              });
              
              // Write the zip to disk
              fs.writeFileSync('artifact.zip', Buffer.from(download.data));
              
              // Extract the artifact
              await exec.exec('unzip -o artifact.zip -d artifact_contents');
              
              return 'success';
            } catch (error) {
              console.log(`Error downloading artifact: ${error.message}`);
              return 'error';
            }
      
      - name: Find recently converted files
        id: find-files
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "Manual trigger - scanning for all markdown files in Markdown Conversions"
            if [ -d "Markdown Conversions" ]; then
              find "Markdown Conversions" -name "*.md" -type f > recently_converted.txt
              if [ -s recently_converted.txt ]; then
                echo "files_found=true" >> $GITHUB_OUTPUT
                echo "Found markdown files for manual processing:"
                cat recently_converted.txt
              else
                echo "files_found=false" >> $GITHUB_OUTPUT
                echo "No markdown files found in Markdown Conversions directory"
              fi
            else
              echo "files_found=false" >> $GITHUB_OUTPUT
              echo "Markdown Conversions directory not found"
            fi
          elif [ "${{ steps.download.outputs.result }}" == "success" ] && [ -f "artifact_contents/converted_files.txt" ]; then
            cp artifact_contents/converted_files.txt recently_converted.txt
            if [ -s recently_converted.txt ]; then
              echo "files_found=true" >> $GITHUB_OUTPUT
              echo "Found recently converted files from artifact:"
              cat recently_converted.txt
            else
              echo "files_found=false" >> $GITHUB_OUTPUT
              echo "Artifact contained empty file list"
            fi
          else
            echo "files_found=false" >> $GITHUB_OUTPUT
            echo "No recently converted files information available"
          fi
      
      - name: Verify summarization prompt exists
        id: check-prompt
        run: |
          PROMPT_PATH=".github/ai-prompts/markdown-summarization-prompt.md"
          if [ -f "$PROMPT_PATH" ]; then
            echo "prompt_exists=true" >> $GITHUB_OUTPUT
            echo "Found summarization prompt at $PROMPT_PATH"
          else
            echo "prompt_exists=false" >> $GITHUB_OUTPUT
            echo "Summarization prompt not found at $PROMPT_PATH"
          fi
      
      - name: Create README update script
        if: steps.find-files.outputs.files_found == 'true' && steps.check-prompt.outputs.prompt_exists == 'true'
        run: |
          cat > update_readme.py << 'ENDPYTHONSCRIPT'
          import os
          import sys
          import re
          import google.generativeai as genai
          from pathlib import Path
          import subprocess
          from typing import Dict, List, Tuple, Optional
          
          # Setup Gemini API
          genai.configure(api_key=os.environ["GEMINI_API_KEY"])
          
          # Define models to try in order of preference
          MODELS = ["gemini-2.5-flash", "gemini-2.0-flash", "gemini-1.5-flash"]
          
          def get_available_model():
              """Find the first available Gemini model that supports 'generateContent'."""
              print("Fetching list of available models...")
              
              # Use genai.list_models() to efficiently get a list of supported models.
              # This avoids making a generation call for each model to check availability.
              for model in genai.list_models():
                  
                  # Check if the model supports the 'generateContent' method, 
                  # which is necessary for typical Gemini API usage.
                  if 'generateContent' in model.supported_generation_methods:
                      print(f"Found available model: {model.name}")
                      return model.name
              
              print("No suitable models available that support 'generateContent'.")
              return None
          
          class ReadmeParser:
              """Parser for extracting dynamic README structure"""
              
              def __init__(self, readme_content: str):
                  self.content = readme_content
                  self.lines = readme_content.split('\n')
                  self.toc_sections = {}
                  self.analysis_subsections = []
                  self.survey_subsections = []
                  
              def parse_toc_structure(self) -> Dict[str, List[str]]:
                  """Parse Table of Contents to extract available sections and subsections"""
                  print("Parsing README Table of Contents structure...")
                  
                  in_toc = False
                  current_main_section = None
                  skip_sections = {"overview", "resources", "contributing", "table of contents"}
                  
                  for line in self.lines:
                      line = line.strip()
                      
                      # Detect TOC start
                      if re.match(r'^#+\s*table\s+of\s+contents', line, re.IGNORECASE):
                          in_toc = True
                          continue
                      
                      # Exit TOC when we hit a new main heading
                      if in_toc and re.match(r'^#+\s+(?!.*\[.*\]\(#)', line):
                          break
                      
                      if in_toc and line:
                          # Main sections (typically - [Section Name](#anchor))
                          main_match = re.match(r'^-\s*\[([^\]]+)\]\(#[^)]+\)', line)
                          if main_match:
                              section_name = main_match.group(1).strip()
                              section_lower = section_name.lower()
                              
                              if section_lower not in skip_sections:
                                  current_main_section = section_name
                                  self.toc_sections[section_name] = []
                                  
                                  # Identify Analysis vs Survey sections
                                  if "analysis" in section_lower:
                                      print(f"Found Analysis section: {section_name}")
                                  elif "survey" in section_lower:
                                      print(f"Found Survey section: {section_name}")
                          
                          # Subsections (typically  - [Subsection](#anchor))
                          elif re.match(r'^\s{2,}-\s*\[([^\]]+)\]\(#[^)]+\)', line):
                              subsection_match = re.match(r'^\s{2,}-\s*\[([^\]]+)\]\(#[^)]+\)', line)
                              if subsection_match and current_main_section:
                                  subsection_name = subsection_match.group(1).strip()
                                  self.toc_sections[current_main_section].append(subsection_name)
                                  
                                  # Store subsections by type for easier access
                                  if current_main_section and "analysis" in current_main_section.lower():
                  self.analysis_subsections.append(subsection_name)
              elif current_main_section and "survey" in current_main_section.lower():
                  self.survey_subsections.append(subsection_name)
      
      print(f"Extracted TOC structure: {self.toc_sections}")
      print(f"Analysis subsections: {self.analysis_subsections}")
      print(f"Survey subsections: {self.survey_subsections}")
      
      return self.toc_sections
  
  def get_existing_pdf_filenames(self) -> set[str]:
      """Extracts original PDF filenames from existing README entries."""
      existing_filenames = set()
      # Regex to capture the original PDF filename from the entry line
      # Example: - [Org](URL) - [Report Title](Annual%20Security%20Reports/YEAR/Original-PDF-Filename.pdf) (YEAR) - Summary
      entry_regex = re.compile(r"- \[.+?\]\(.+?\) - \[.+?\]\(Annual%20Security%20Reports/\d{4}/(.+?)\) \(\d{4}\) - .+")
      
      for line in self.lines:
          match = entry_regex.search(line)
          if match:
              existing_filenames.add(match.group(1))
      print(f"Found {len(existing_filenames)} existing PDF filenames in README.")
      return existing_filenames

  def find_section_content_area(self, section_heading: str) -> Tuple[int, int]:
      """Find the line range for a specific section in the README"""
      start_line = -1
      end_line = len(self.lines)
      
      # Create pattern to match the section heading
      section_pattern = re.compile(r'^#+\s+' + re.escape(section_heading), re.IGNORECASE)
      
      for i, line in enumerate(self.lines):
          if section_pattern.match(line.strip()):
              start_line = i
              break
      
      if start_line == -1:
          return -1, -1
      
      # Find the end of this section (next heading of same level or higher)
      section_level = len(re.match(r'^#+', self.lines[start_line]).group())
      
      for i in range(start_line + 1, len(self.lines)):
          line = self.lines[i].strip()
          if re.match(r'^#+', line):
              current_level = len(re.match(r'^#+', line).group())
              if current_level <= section_level:
                  end_line = i
                  break
      
      return start_line, end_line
          
          class ReportAnalyzer:
              """AI-powered report content analyzer"""
              
              def __init__(self, model_name: str, summarization_prompt: str):
                  self.model = genai.GenerativeModel(model_name)
                  self.summarization_prompt = summarization_prompt
                  self.keyword_map = {
                      "identity": ["identity", "authentication", "iam", "access control"],
                      "cloud": ["cloud", "aws", "azure", "gcp", "kubernetes", "container"],
                      "application": ["api", "app", "application", "software", "web", "mobile"],
                      "ransomware": ["ransomware", "malware", "trojan", "virus"],
                      "vulnerability": ["vulnerability", "cve", "exploit", "patch", "zero-day"],
                      "privacy": ["privacy", "gdpr", "data protection", "compliance", "regulation"],
                      "breach": ["breach", "incident", "compromise", "leak", "exposure"],
                      "physical": ["physical", "facility", "building", "device"],
                      "ai": ["ai", "artificial intelligence", "ml", "machine learning", "automation"],
                      "threat": ["threat", "apt", "attack", "campaign", "actor"],
                      "trends": ["trends", "industry", "market", "survey", "statistics"],
                      "penetration": ["penetration", "pentest", "red team", "ethical hacking"]
                  }
              
              def extract_org_and_year(self, file_path: str) -> Tuple[str, str, str, str]:
                  """Extract organization name, year, report title, and original PDF filename from file path"""
                  path = Path(file_path)
                  
                  # Extract year from path (e.g., "Markdown Conversions/2025/...")
                  year = "Unknown"
                  for part in path.parts:
                      if re.match(r'^\d{4}$', part):
                          year = part
                          break
                  
                  # Extract organization and full filename stem
                  filename_stem = path.stem # e.g., BlackDuck-Software-Vulnerability-Snapshot-Report-2024
                  org_match = re.match(r'^([^-]+)', filename_stem)
                  org = org_match.group(1) if org_match else filename_stem
                  
                  # Derive report title
                  report_title_raw = filename_stem.replace(f"{org}-", "").replace(f"-{year}", "")
                  report_title = report_title_raw.replace("-", " ").strip()
                  
                  # Derive original PDF filename
                  original_pdf_filename = f"{filename_stem}.pdf"
                  
                  return org.strip(), year, report_title, original_pdf_filename
              
              def read_markdown_content(self, file_path: str) -> str:
                  """Read the content of a markdown file"""
                  try:
                      with open(file_path, 'r', encoding='utf-8') as f:
                          return f.read()
                  except Exception as e:
                      print(f"Error reading {file_path}: {str(e)}")
                      return ""
              
              def generate_summary(self, content: str) -> str:
                  """Generate a concise summary using AI"""
                  try:
                      full_prompt = f"{self.summarization_prompt}\n\n# Report Content Below\n\n{content[:8000]}"  # Limit content
                      
                      response = self.model.generate_content(
                          full_prompt,
                          generation_config={
                              "temperature": 0.2,
                              "max_output_tokens": 500
                          }
                      )
                      
                      return response.text.strip()
                  except Exception as e:
                      print(f"Error generating summary: {str(e)}")
                      return "Error generating summary with AI."
              
              def determine_report_type(self, content: str, filename: str) -> str:
                  """Determine if report is Analysis or Survey focused"""
                  try:
                      prompt = f"""
                      Analyze this security report and determine if it is primarily:
                      1. ANALYSIS-FOCUSED: Technical research, threat analysis, vulnerability studies, detailed technical findings
                      2. SURVEY-FOCUSED: Industry trends, statistics, market research, survey results, benchmarking
                      
                      Respond with exactly one word: "ANALYSIS" or "SURVEY"
                      
                      Report filename: {filename}
                      Report content (first 4000 chars): {content[:4000]}
                      """
                      
                      response = self.model.generate_content(
                          prompt,
                          generation_config={
                              "temperature": 0.1,
                              "max_output_tokens": 10
                          }
                      )
                      
                      result = response.text.strip().upper()
                      if result in ["ANALYSIS", "SURVEY"]:
                          return result
                      else:
                          # Default fallback based on keywords
                          content_lower = content.lower()
                          analysis_keywords = ["vulnerability", "exploit", "attack", "threat", "technical", "research"]
                          survey_keywords = ["survey", "trends", "industry", "statistics", "market", "benchmark"]
                          
                          analysis_score = sum(1 for kw in analysis_keywords if kw in content_lower)
                          survey_score = sum(1 for kw in survey_keywords if kw in content_lower)
                          
                          return "ANALYSIS" if analysis_score >= survey_score else "SURVEY"
                  
                  except Exception as e:
                      print(f"Error determining report type: {str(e)}")
                      return "ANALYSIS"  # Default fallback
              
              def find_best_subsection(self, content: str, available_subsections: List[str]) -> str:
                  """Find the best matching subsection for the report"""
                  if not available_subsections:
                      return available_subsections[0] if available_subsections else "General"
                  
                  content_lower = content.lower()
                  best_match = available_subsections[0]  # Default to first available
                  best_score = 0
                  
                  for subsection in available_subsections:
                      score = 0
                      subsection_lower = subsection.lower()
                      
                      # Direct keyword matching
                      for category, keywords in self.keyword_map.items():
                          if any(keyword in subsection_lower for keyword in keywords):
                              # Count how many of these keywords appear in content
                              keyword_count = sum(1 for kw in keywords if kw in content_lower)
                              score += keyword_count * 2  # Weight keyword matches highly
                      
                      # Partial name matching
                      subsection_words = re.findall(r'\w+', subsection_lower)
                      for word in subsection_words:
                          if len(word) > 3 and word in content_lower:
                              score += 1
                      
                      if score > best_score:
                          best_score = score
                          best_match = subsection
                  
                  print(f"Best subsection match: {best_match} (score: {best_score})")
                  return best_match
          
          class ReadmeUpdater:
              """Updates README.md with new report entries"""
              
              def __init__(self, readme_content: str):
                  self.content = readme_content
                  self.lines = readme_content.split('\n')
              
              def add_report_entry(self, main_section: str, subsection: str, org: str, year: str, 
                                 publisher_url: str, report_title: str, original_pdf_filename: str, summary: str) -> str:
                  """Add a new report entry to the README"""
                  
                  # Create the new entry
                  # New format: - [<PublisherName>](<PublisherURL>) - [<ReportTitle>](<LocalFilePath>) (<Year>) - <Summary>
                  local_file_path = f"Annual%20Security%20Reports/{year}/{original_pdf_filename}"
                  entry_line = f"- [{org}]({publisher_url}) - [{report_title}]({local_file_path}) ({year}) - {summary}"
                  
                  # Find the subsection
                  subsection_pattern = re.compile(r'^#+\s+' + re.escape(subsection), re.IGNORECASE)
                  subsection_line = -1
                  
                  for i, line in enumerate(self.lines):
                      if subsection_pattern.match(line.strip()):
                          subsection_line = i
                          break
                  
                  if subsection_line == -1:
                      print(f"Warning: Could not find subsection {subsection}")
                      return self.content
                  
                  # Find insertion point (after subsection heading but before next section)
                  insert_line = subsection_line + 1
                  
                  # Skip any existing content until we find the right spot
                  next_section_line = len(self.lines)
                  for i in range(subsection_line + 1, len(self.lines)):
                      line = self.lines[i].strip()
                      if re.match(r'^#+', line):  # Found next section
                          next_section_line = i
                          break
                  
                  # Find existing entries and insert alphabetically
                  entries = []
                  entry_lines = []
                  
                  # Regex to match existing entries in the new format for replacement check
                  # This regex looks for a line starting with `- [` followed by the organization name,
                  # then any characters (`.*?`), then `(` followed by the year, then `) - `.
                  org_year_regex = re.compile(r"^- \[" + re.escape(org) + r"\].*?\(" + re.escape(year) + r"\) - ")
                  
                  for i in range(subsection_line + 1, next_section_line):
                      line = self.lines[i].strip()
                      # Check if this line is an existing report entry in the new format
                      if re.match(r"^- \[.+?\]\(.+?\) - \[.+?\]\(.+?\) \(\d{4}\) - .+", line):
                          entries.append(line)
                          entry_lines.append(i)
                  
                  # Check if this entry already exists (same org and year)
                  for i, existing_entry_line_content in enumerate(self.lines):
                      if org_year_regex.match(existing_entry_line_content.strip()):
                          print(f"Entry for {org} {year} already exists, updating...")
                          self.lines[i] = entry_line
                          return '\n'.join(self.lines)
                  
                  # Add new entry in alphabetical order
                  entries.append(entry_line)
                  entries.sort(key=lambda x: x.lower())
                  
                  # Find where to insert
                  entry_index = entries.index(entry_line)
                  
                  if entry_lines:
                      if entry_index == 0:
                          insert_at = entry_lines[0]
                      elif entry_index >= len(entry_lines):
                          insert_at = entry_lines[-1] + 1
                      else:
                          insert_at = entry_lines[entry_index - 1] + 1
                  else:
                      # No existing entries, insert after subsection heading
                      insert_at = subsection_line + 1
                  
                  # Insert the new entry
                  self.lines.insert(insert_at, entry_line)
                  
                  return '\n'.join(self.lines)
              
              def get_updated_content(self) -> str:
                  """Get the updated README content"""
                  return '\n'.join(self.lines)
          
          def main():
              if len(sys.argv) < 3:
                  print("Usage: python update_readme.py <files_list> <summarization_prompt_path>")
                  return 1
              
              files_list_path = sys.argv[1]
              prompt_path = sys.argv[2]
              
              # Get available model
              model_name = get_available_model()
              if not model_name:
                  print("No Gemini models available")
                  return 1
              
              # Read files list
              try:
                  with open(files_list_path, 'r') as f:
                      file_paths = [line.strip() for line in f.readlines() if line.strip()]
              except Exception as e:
                  print(f"Error reading files list: {str(e)}")
                  return 1
              
              if not file_paths:
                  print("No files to process")
                  return 0
              
              # Read summarization prompt
              try:
                  with open(prompt_path, 'r') as f:
                      summarization_prompt = f.read()
              except Exception as e:
                  print(f"Error reading summarization prompt: {str(e)}")
                  return 1
              
              # Read current README
              try:
                  with open('README.md', 'r') as f:
                      readme_content = f.read()
              except Exception as e:
                  print(f"Error reading README.md: {str(e)}")
                  return 1
              
              # Initialize components
              parser = ReadmeParser(readme_content)
              toc_structure = parser.parse_toc_structure()
              existing_pdf_filenames = parser.get_existing_pdf_filenames() # Get existing filenames
              
              analyzer = ReportAnalyzer(model_name, summarization_prompt)
              updater = ReadmeUpdater(readme_content)
              
              updated_orgs = []
              
              print(f"Processing {len(file_paths)} files...")
              
              # Process each file
              for file_path in file_paths:
                  try:
                      print(f"\nProcessing: {file_path}")
                      
                      if not os.path.exists(file_path):
                          print(f"File not found: {file_path}")
                          continue
                      
                      # Extract org, year, report title, and original PDF filename
                      org, year, report_title, original_pdf_filename = analyzer.extract_org_and_year(file_path)
                      print(f"Organization: {org}, Year: {year}, Report Title: {report_title}, Original PDF: {original_pdf_filename}")
                      
                      # Skip if this file (based on its original PDF filename) is already in README
                      if original_pdf_filename in existing_pdf_filenames:
                          print(f"Skipping already processed file: {file_path} (Entry for {org} {year} already exists)")
                          continue
                      
                      # Read content
                      content = analyzer.read_markdown_content(file_path)
                      if not content:
                          continue
                      
                      # Generate summary
                      summary = analyzer.generate_summary(content)
                      print(f"Generated summary: {summary[:100]}...")
                      
                      # Determine report type
                      report_type = analyzer.determine_report_type(content, Path(file_path).name)
                      print(f"Report type: {report_type}")
                      
                      # Select appropriate subsection
                      if report_type == "ANALYSIS":
                          available_subsections = parser.analysis_subsections
                          main_section = next((s for s in toc_structure.keys() if "analysis" in s.lower()), "Analysis Reports")
                      else:
                          available_subsections = parser.survey_subsections
                          main_section = next((s for s in toc_structure.keys() if "survey" in s.lower()), "Survey Reports")
                      
                      if not available_subsections:
                          print(f"No subsections found for {report_type} reports")
                          continue
                      
                      best_subsection = analyzer.find_best_subsection(content, available_subsections)
                      print(f"Selected subsection: {best_subsection}")
                      
                      # Placeholder for PublisherURL as it cannot be reliably derived from current information
                      publisher_url = f"https://[PublisherWebsite]" 
                      
                      # Update README
                      updated_content = updater.add_report_entry(
                          main_section, best_subsection, org, year, publisher_url, report_title, original_pdf_filename, summary
                      )
                      updater = ReadmeUpdater(updated_content)  # Update with new content
                      
                      updated_orgs.append(f"{org} {year}")
                      
                  except Exception as e:
                      print(f"Error processing {file_path}: {str(e)}")
                      continue
              
              if updated_orgs:
                  # Write updated README
                  try:
                      with open('README.md', 'w') as f:
                          f.write(updater.get_updated_content())
                      print(f"README.md updated successfully")
                      
                      # Write summary for PR
                      with open('pr_summary.txt', 'w') as f:
                          f.write(f"Updated README with {len(updated_orgs)} reports:\n")
                          for org in updated_orgs:
                              f.write(f"- {org}\n")
                      
                      return 0
                  except Exception as e:
                      print(f"Error writing README: {str(e)}")
                      return 1
              else:
                  print("No files were successfully processed")
                  return 1
          
          if __name__ == "__main__":
              sys.exit(main())
          ENDPYTHONSCRIPT
      
      - name: Update README with AI
        if: steps.find-files.outputs.files_found == 'true' && steps.check-prompt.outputs.prompt_exists == 'true'
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python update_readme.py recently_converted.txt ".github/ai-prompts/markdown-summarization-prompt.md"
      
      - name: Create Pull Request
        if: steps.find-files.outputs.files_found == 'true' && steps.check-prompt.outputs.prompt_exists == 'true'
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "Update README with new security reports"
          title: "📊 Update README with New Security Reports"
          body-path: pr_summary.txt
          branch: readme-update-${{ github.run_number }}
          delete-branch: true
          base: ${{ steps.branch.outputs.branch }}
      
      - name: Upload debug artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: readme-update-debug-${{ github.run_number }}
          path: |
            recently_converted.txt
            pr_summary.txt
            artifact_contents/
          retention-days: 5
      
      - name: Summary
        if: always()
        run: |
          echo "## README Update Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.find-files.outputs.files_found }}" == "true" ]; then
            echo "✅ Found files to process" >> $GITHUB_STEP_SUMMARY
            if [ -f "pr_summary.txt" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### Updated Reports:" >> $GITHUB_STEP_SUMMARY
              cat pr_summary.txt >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "ℹ️ No files found to process" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ steps.check-prompt.outputs.prompt_exists }}" != "true" ]; then
            echo "❌ Summarization prompt not found" >> $GITHUB_STEP_SUMMARY
