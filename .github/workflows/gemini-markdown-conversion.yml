name: Gemini Markdown Conversion

on:
  workflow_dispatch:
    inputs:
      pdf_path:
        description: 'Relative path to the clean PDF file to convert'
        required: true
        type: string

# Prevent multiple runs for the same PDF concurrently, useful if trigger fires rapidly
concurrency:
  group: ${{ github.workflow }}-${{ github.event.inputs.pdf_path }}
  cancel-in-progress: true

jobs:
  convert-to-markdown:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Needed to commit the new Markdown file

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.REPO_ACCESS_TOKEN }} # Use PAT for commit permission
          ref: development # Ensure we checkout the correct branch

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # Specify a Python version

      - name: Install Google Generative AI library
        run: pip install google-generativeai

      - name: Fetch AI Prompt Content
        id: get-prompt
        run: |
          prompt_file="AI_PROMPT_MARKDOWN_CONVERT.md"
          if [[ ! -f "$prompt_file" ]]; then
             echo "::error::AI Prompt file not found at $prompt_file"
             exit 1
          fi
          # Read file content and make it available to the python script env var
          # Use process substitution and export to handle multiline/special chars
          export AI_PROMPT_CONTENT=$(<"$prompt_file")
          echo "AI Prompt content loaded."
          # Optional: Output prompt to logs for verification (be careful with sensitive info if any)
          # echo "Prompt: $AI_PROMPT_CONTENT"

      - name: Convert PDF to Markdown using Gemini
        id: gemini-convert
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          PDF_PATH: ${{ github.event.inputs.pdf_path }}
          AI_PROMPT: ${{ env.AI_PROMPT_CONTENT }} # Pass prompt content via env var
          MODEL_PRIMARY: 'gemini-2.0-flash' # User specified primary model
          MODEL_FALLBACK: 'gemini-2.0-flash-lite' # User specified fallback model
        run: |
          # Python script to interact with Gemini API
          # Note: Ensure the google-generativeai library supports PDF uploads directly.
          # As of recent versions, it uses the File API for this.

          cat << EOF > gemini_script.py
          import google.generativeai as genai
          import os
          import time
          import sys

          # --- Configuration ---
          API_KEY = os.getenv('GEMINI_API_KEY')
          PDF_PATH = os.getenv('PDF_PATH')
          PROMPT = os.getenv('AI_PROMPT')
          MODEL_PRIMARY = os.getenv('MODEL_PRIMARY')
          MODEL_FALLBACK = os.getenv('MODEL_FALLBACK')
          OUTPUT_MD_FILE = "conversion_result.md"
          OUTPUT_MODEL_FILE = "used_model.txt"

          if not all([API_KEY, PDF_PATH, PROMPT, MODEL_PRIMARY, MODEL_FALLBACK]):
              print("::error::Missing required environment variables (API_KEY, PDF_PATH, PROMPT, MODELS)")
              sys.exit(1)

          genai.configure(api_key=API_KEY)

          # --- File Upload ---
          pdf_file_resource = None
          try:
              print(f"Uploading file: {PDF_PATH}...")
              pdf_file_resource = genai.upload_file(path=PDF_PATH, display_name=os.path.basename(PDF_PATH))
              print(f"Upload initiated. File resource name: {pdf_file_resource.name}")

              # --- Wait for File Processing ---
              print("Waiting for file processing...")
              while pdf_file_resource.state.name == "PROCESSING":
                  print('.', end='', flush=True)
                  time.sleep(5) # Check every 5 seconds
                  # Make sure to get the updated file resource state
                  pdf_file_resource = genai.get_file(pdf_file_resource.name)

              if pdf_file_resource.state.name != "ACTIVE":
                  print(f"\n::error::File processing failed. Final state: {pdf_file_resource.state.name}")
                  # Attempt cleanup before exiting
                  try:
                      genai.delete_file(pdf_file_resource.name)
                      print(f"Cleaned up uploaded file: {pdf_file_resource.name}")
                  except Exception as e_del:
                      print(f"::warning::Failed to delete uploaded file {pdf_file_resource.name} after processing failure: {e_del}")
                  sys.exit(1)

              print("\nFile is now ACTIVE.")

          except Exception as e:
              print(f"::error::An error occurred during file upload or processing: {e}")
              # If file resource exists but failed during processing check, try to delete
              if pdf_file_resource and pdf_file_resource.name:
                  try:
                      genai.delete_file(pdf_file_resource.name)
                      print(f"Cleaned up uploaded file resource {pdf_file_resource.name} after error.")
                  except Exception as e_del:
                      print(f"::warning::Failed to delete potentially uploaded file {pdf_file_resource.name} after error: {e_del}")
              sys.exit(1)


          # --- Content Generation (with failover) ---
          markdown_content = None
          used_model = ""
          models_to_try = [MODEL_PRIMARY, MODEL_FALLBACK]

          for model_name in models_to_try:
              try:
                  print(f"Attempting conversion with model: {model_name}...")
                  model = genai.GenerativeModel(model_name)
                  # Prepare content: Prompt text + Uploaded File object
                  # The library handles constructing the request from this list
                  request_content = [PROMPT, pdf_file_resource]

                  response = model.generate_content(request_content)

                  # Check for empty or blocked response
                  if not response.parts:
                     if response.prompt_feedback.block_reason:
                         print(f"::error::Model {model_name} blocked the prompt. Reason: {response.prompt_feedback.block_reason}")
                     else:
                         print(f"::error::Model {model_name} returned an empty response.")
                     # Continue to next model (if any) or fail
                     continue # Go to next model in the loop

                  markdown_content = response.text
                  used_model = model_name
                  print(f"Successfully converted using {used_model}")
                  break # Success, exit the loop

              except Exception as e:
                  print(f"::warning::Model {model_name} failed: {e}")
                  # If this was the last model, the loop will end, and we'll check markdown_content

          # --- Cleanup Uploaded File ---
          try:
              print(f"Deleting uploaded file: {pdf_file_resource.name}")
              genai.delete_file(pdf_file_resource.name)
          except Exception as e_delete:
               print(f"::warning::Failed to delete uploaded file {pdf_file_resource.name}: {e_delete}")

          # --- Check result and Write Output ---
          if markdown_content:
              print(f"Writing Markdown content to {OUTPUT_MD_FILE}")
              with open(OUTPUT_MD_FILE, "w", encoding='utf-8') as f:
                  f.write(markdown_content)
              with open(OUTPUT_MODEL_FILE, "w") as f:
                  f.write(used_model)
              print("Conversion script finished successfully.")
          else:
              print("::error::Both Gemini models failed to generate content.")
              sys.exit(1)
          EOF

          # Execute the Python script
          python gemini_script.py

      - name: Determine Output Path and Commit Markdown
        run: |
          if [[ ! -f conversion_result.md || ! -f used_model.txt ]]; then
            echo "::error::Conversion output files not found. Gemini step likely failed."
            exit 1
          fi

          pdf_path="${{ github.event.inputs.pdf_path }}"
          # Get directory from PDF path: Annual Security Reports/YYYY -> Markdown Conversions/YYYY
          pdf_dir=$(dirname "$pdf_path")
          target_subdir=$(echo "$pdf_dir" | sed 's|Annual Security Reports|Markdown Conversions|')

          # Get base filename without extension: file.pdf -> file
          pdf_filename_base=$(basename "$pdf_path" .pdf)
          # Construct target Markdown path
          target_md_file="${target_subdir}/${pdf_filename_base}.md"
          target_md_dir=$(dirname "$target_md_file")

          echo "Original PDF path: $pdf_path"
          echo "Target Markdown directory: $target_md_dir"
          echo "Target Markdown file: $target_md_file"

          # Create target directory if it doesn't exist
          mkdir -p "$target_md_dir"

          # Move the result file to the correct location
          mv conversion_result.md "$target_md_file"
          used_model=$(cat used_model.txt)
          rm used_model.txt # Clean up model file

          # Configure Git
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'

          # Add, Commit, and Push
          git add "$target_md_file"

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes detected in $target_md_file. Skipping commit."
          else
            echo "Committing $target_md_file..."
            commit_message="feat: Convert $(basename "$pdf_path") to Markdown

            Processed using AI Prompt V1.0, Model: ${used_model}"

            git commit -m "$commit_message"
            echo "Pushing changes to development branch..."
            git push origin development
            if [[ $? -ne 0 ]]; then
                echo "::error::Failed to push changes to the repository."
                exit 1
            fi
            echo "Changes pushed successfully."
          fi

      - name: Clean up Python script
        if: always() # Run even if previous steps fail
        run: rm -f gemini_script.py
